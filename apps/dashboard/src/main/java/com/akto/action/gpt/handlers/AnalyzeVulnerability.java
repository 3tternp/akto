package com.akto.action.gpt.handlers;

import com.akto.action.gpt.GptAction;
import com.akto.action.gpt.result_fetchers.ResultFetcherStrategy;
import com.mongodb.BasicDBObject;
import com.mongodb.BasicDBList;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class AnalyzeVulnerability implements QueryHandler {
    
    private static final Logger logger = LoggerFactory.getLogger(AnalyzeVulnerability.class);
    private final ResultFetcherStrategy<BasicDBObject> resultFetcherStrategy;
    
    public AnalyzeVulnerability(ResultFetcherStrategy<BasicDBObject> resultFetcherStrategy) {
        this.resultFetcherStrategy = resultFetcherStrategy;
    }
    
    @Override
    public BasicDBObject handleQuery(BasicDBObject meta) throws Exception {
        BasicDBObject request = new BasicDBObject();
        
        String responseOutput = meta.getString("responseOutput");
        String vulnerabilityType = meta.getString("vulnerabilityType");
        
        request.put("query_type", GptQuery.ANALYZE_VULNERABILITY.getName());
        request.put("response_output", responseOutput);
        request.put("vulnerability_type", vulnerabilityType);
        request.put(GptAction.USER_EMAIL, meta.getString(GptAction.USER_EMAIL));
        
        // Create the prompt for LLM
        String prompt = buildPrompt(responseOutput, vulnerabilityType);
        request.put("prompt", prompt);
        
        logger.info("Analyzing response for {} vulnerabilities", vulnerabilityType);
        
        // TODO: Remove this dummy response when LLM integration is working
        boolean USE_DUMMY_RESPONSE = true; // Set to false when LLM is working
        BasicDBObject llmResponse;
        
        if (USE_DUMMY_RESPONSE) {
            llmResponse = createDummyResponse(responseOutput, vulnerabilityType);
        } else {
            // Call LLM to analyze the response
            llmResponse = this.resultFetcherStrategy.fetchResult(request);
        }
        
        // Process and format the LLM response
        BasicDBObject result = processLLMResponse(llmResponse, responseOutput, vulnerabilityType);

        logger.info("Returning vulnerability analysis result with {} vulnerable segments",
                   result.get("vulnerableSegments") != null ? ((BasicDBList)result.get("vulnerableSegments")).size() : 0);

        return result;
    }
    
    private String buildPrompt(String responseOutput, String vulnerabilityType) {
        StringBuilder prompt = new StringBuilder();
        prompt.append("Analyze the following HTTP response for ").append(vulnerabilityType).append(" vulnerabilities.\n\n");
        prompt.append("Response to analyze:\n").append(responseOutput).append("\n\n");
        
        prompt.append("Instructions:\n");
        prompt.append("1. Identify specific patterns or content that indicate ").append(vulnerabilityType).append(" vulnerabilities\n");
        prompt.append("2. For each vulnerability found, provide the start and end position in the response (character index)\n");
        prompt.append("3. Return the analysis in JSON format with structure:\n");
        prompt.append("{\n");
        prompt.append("  \"isVulnerable\": boolean,\n");
        prompt.append("  \"vulnerableSegments\": [\n");
        prompt.append("    {\n");
        prompt.append("      \"start\": number,\n");
        prompt.append("      \"end\": number\n");
        prompt.append("    }\n");
        prompt.append("  ]\n");
        prompt.append("}\n\n");
        
        // Add specific guidance based on vulnerability type
        switch (vulnerabilityType.toUpperCase()) {
            case "XSS":
                prompt.append("Focus on: script tags, javascript: URIs, event handlers, eval(), document.cookie, document.write, etc.\n");
                break;
            case "SQL_INJECTION":
                prompt.append("Focus on: SQL error messages, database error patterns, SQL syntax errors, database vendor information.\n");
                break;
            case "SENSITIVE_DATA":
                prompt.append("Focus on: emails, credit cards, SSNs, API keys, tokens, passwords, private keys. Mask sensitive data in response.\n");
                break;
            case "ERROR_DISCLOSURE":
                prompt.append("Focus on: stack traces, exception messages, internal paths, framework details, debug information.\n");
                break;
            case "PATH_TRAVERSAL":
                prompt.append("Focus on: ../ patterns, file paths, directory listings, system file contents.\n");
                break;
            default:
                prompt.append("Look for any security-relevant patterns or sensitive information.\n");
        }
        
        return prompt.toString();
    }
    
    private BasicDBObject createDummyResponse(String responseOutput, String vulnerabilityType) {
        BasicDBObject dummyResponse = new BasicDBObject();
        BasicDBList vulnerableSegments = new BasicDBList();

        logger.info("Creating dummy response for vulnerability type: {} with response length: {}",
                   vulnerabilityType, responseOutput != null ? responseOutput.length() : 0);

        // Always add at least one test vulnerability for demonstration
        if (responseOutput != null && responseOutput.length() > 10) {
            BasicDBObject testSegment = new BasicDBObject();
            testSegment.put("start", 5);
            testSegment.put("end", Math.min(50, responseOutput.length()));
            vulnerableSegments.add(testSegment);
            logger.info("Added test segment: start=5, end={}", Math.min(50, responseOutput.length()));
        }
        
        // Add some dummy vulnerable segments based on common patterns
        if (responseOutput != null && vulnerabilityType.equals("XSS")) {
            // Look for script tags or common XSS patterns
            if (responseOutput.contains("<script>")) {
                int start = responseOutput.indexOf("<script>");
                int end = responseOutput.indexOf("</script>") + 9;
                if (end > start) {
                    BasicDBObject segment = new BasicDBObject();
                    segment.put("start", start);
                    segment.put("end", Math.min(end, responseOutput.length()));
                    vulnerableSegments.add(segment);
                }
            }
        } else if (responseOutput != null && vulnerabilityType.equals("SQL_INJECTION")) {
            // Look for SQL error patterns
            String[] sqlPatterns = {"SQL syntax", "mysql_fetch", "ORA-", "PostgreSQL"};
            for (String pattern : sqlPatterns) {
                int index = responseOutput.toLowerCase().indexOf(pattern.toLowerCase());
                if (index != -1) {
                    BasicDBObject segment = new BasicDBObject();
                    segment.put("start", index);
                    segment.put("end", Math.min(index + 50, responseOutput.length()));
                    vulnerableSegments.add(segment);
                    break;
                }
            }
        } else if (responseOutput != null && vulnerabilityType.equals("SENSITIVE_DATA")) {
            // Look for email patterns
            java.util.regex.Pattern emailPattern = java.util.regex.Pattern.compile("[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}");
            java.util.regex.Matcher matcher = emailPattern.matcher(responseOutput);
            if (matcher.find()) {
                BasicDBObject segment = new BasicDBObject();
                segment.put("start", matcher.start());
                segment.put("end", matcher.end());
                vulnerableSegments.add(segment);
            }
        }
        
        // Create dummy LLM response format as a proper JSON object
        BasicDBObject jsonResponse = new BasicDBObject();
        jsonResponse.put("isVulnerable", vulnerableSegments.size() > 0);
        jsonResponse.put("vulnerableSegments", vulnerableSegments);

        logger.info("Created dummy response with {} vulnerable segments", vulnerableSegments.size());

        // Convert to JSON string for processing
        dummyResponse.put("response", jsonResponse.toJson());
        return dummyResponse;
    }
    
    private BasicDBObject processLLMResponse(BasicDBObject llmResponse, String originalResponse, String vulnerabilityType) {
        BasicDBObject result = new BasicDBObject();
        
        try {
            // Parse the LLM response - it should contain the vulnerability analysis
            if (llmResponse != null && llmResponse.containsField("response")) {
                String llmResponseStr = llmResponse.getString("response");
                
                // Try to parse the JSON response from LLM
                try {
                    BasicDBObject parsedResponse = BasicDBObject.parse(llmResponseStr);
                    
                    // Add vulnerability type to the result
                    result.put("vulnerabilityType", vulnerabilityType);
                    result.put("isVulnerable", parsedResponse.getBoolean("isVulnerable", false));
                    
                    // Process vulnerable segments
                    BasicDBList segments = (BasicDBList) parsedResponse.get("vulnerableSegments");
                    if (segments == null) {
                        segments = new BasicDBList();
                    }
                    
                    result.put("vulnerableSegments", segments);
                    
                } catch (Exception e) {
                    logger.error("Failed to parse LLM JSON response, using fallback", e);
                    // Fallback: create a simple response if JSON parsing fails
                    result.put("vulnerabilityType", vulnerabilityType);
                    result.put("isVulnerable", false);
                    result.put("vulnerableSegments", new BasicDBList());
                    result.put("error", "Failed to parse LLM response");
                }
            } else {
                // No response from LLM
                result.put("vulnerabilityType", vulnerabilityType);
                result.put("isVulnerable", false);
                result.put("vulnerableSegments", new BasicDBList());
                result.put("error", "No response from LLM");
            }
            
        } catch (Exception e) {
            logger.error("Error processing LLM response", e);
            result.put("error", e.getMessage());
            result.put("isVulnerable", false);
            result.put("vulnerableSegments", new BasicDBList());
        }
        
        return result;
    }
}